{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Exam Assignment (40 points - Total of 45 is Possible)\n",
    "## Due December 7, 2022, @ 8:00 am\n",
    "Note that there will be no extensions given for this assignment as there is a tight timeline for grading. \n",
    "\n",
    "For this assignment, I have provided each of you with your own training dataset. Your goal is to train a deep neural network to uncover the code image provided to you. \n",
    "\n",
    "I will provide you with instructions throughout. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from urllib.request import urlopen\n",
    "from urllib.parse import urlencode\n",
    "import urllib.request\n",
    "\n",
    "import time\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "# This is a tool I have provided you to help you download your file.\n",
    "\n",
    "def download_file(url, filename):\n",
    "    \"\"\"\n",
    "    A function that downloads the data file from a URL\n",
    "    Parameters\n",
    "    ----------\n",
    "    url : string\n",
    "        url where the file to download is located\n",
    "    filename : string\n",
    "        location where to save the file\n",
    "    reporthook : function\n",
    "        callback to display the download progress\n",
    "    \"\"\"\n",
    "    if not os.path.isfile(filename):\n",
    "        urllib.request.urlretrieve(url, filename, reporthook)\n",
    "        \n",
    "def reporthook(count, block_size, total_size):\n",
    "    \"\"\"\n",
    "    A function that displays the status and speed of the download\n",
    "    \"\"\"\n",
    "\n",
    "    global start_time\n",
    "    if count == 0:\n",
    "        start_time = time.time()\n",
    "        return\n",
    "    duration = time.time() - start_time\n",
    "    progress_size = int(count * block_size)\n",
    "    speed = int(progress_size / (1024 * duration + 0.0001))\n",
    "    percent = int(count * block_size * 100 / total_size)\n",
    "    sys.stdout.write(\"\\r...%d%%, %d MB, %d KB/s, %d seconds passed\" %\n",
    "                     (percent, progress_size / (1024 * 1024), speed, duration))\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "# You can download your file by typing your first name into the name block\n",
    "# The name used is the first part of your first name as listed in BB learn\n",
    "# If you have problems downloading the data please reach out to me\n",
    "\n",
    "name = 'Engin'\n",
    "download_file(f'https://zenodo.org/record/7339649/files/data_{name}.npz?download=1','data.npz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Data (3 points)\n",
    "The data is provided to you as a compressed NumPy array saved as 'data.npz'. When working with real data you might need to figure out how data is stored. Use the information on 'npz' files to figure out what data you have. The data file contains three NumPy arrays. \n",
    "1. The features for the training dataset\n",
    "2. The regression values for the training dataset\n",
    "3. The validation features that contain your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['training_feat', 'training_true', 'validation_feat']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=np.load(\"data.npz\")\n",
    "df.files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with np.load('data.npz') as data:\n",
    "    training_feat = data['training_feat']\n",
    "    training_true = data['training_true']\n",
    "    validation_feat = data['validation_feat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_feat:(100000, 30)\n",
      "training_true:(100000, 3) \n",
      "validation_feat:(65536, 30)\n"
     ]
    }
   ],
   "source": [
    "print(f\"training_feat:{training_feat.shape}\\ntraining_true:{training_true.shape} \\nvalidation_feat:{validation_feat.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[21.47218478, 90.04014005, 11.05126661, ..., 84.26812396,\n",
       "         3.69941841,  9.40658226],\n",
       "       [34.59418053, 86.13075664, 59.7060796 , ...,  4.05397443,\n",
       "        14.66966508,  7.93387018],\n",
       "       [13.52182056, 50.00514596, 79.03533638, ..., 32.93895487,\n",
       "        11.08870387, 30.80815178],\n",
       "       ...,\n",
       "       [24.15637886, 34.66605913, 20.25588829, ..., 18.97459701,\n",
       "        37.61646917, 29.57489185],\n",
       "       [ 4.50644774, 23.09436806, 44.70916402, ...,  1.02775268,\n",
       "        25.50172331, 15.28646073],\n",
       "       [12.28109588, 64.9850014 , 32.62234029, ..., 71.40155902,\n",
       "         5.48593596, 18.74249612]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_feat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the Data (5 points)\n",
    "\n",
    "You should explore the data and figure out the best way to preprocess the data. \n",
    "\n",
    "Hints: \n",
    "1. For the regression values, these at the end will represent colors in RGB space from [0,1]. It is recommended to use a max-min scalar between 0 and 1. \n",
    "2. For the training features, you should look at the data and determine the best scaling method. Look at our class notes for a reminder of what other scaler might be useful. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>19.782907</td>\n",
       "      <td>46.090433</td>\n",
       "      <td>42.772808</td>\n",
       "      <td>8.126919</td>\n",
       "      <td>39.682211</td>\n",
       "      <td>12.045670</td>\n",
       "      <td>43.537720</td>\n",
       "      <td>17.445476</td>\n",
       "      <td>19.697835</td>\n",
       "      <td>6.843953</td>\n",
       "      <td>...</td>\n",
       "      <td>25.922545</td>\n",
       "      <td>0.877013</td>\n",
       "      <td>39.781936</td>\n",
       "      <td>34.854689</td>\n",
       "      <td>25.343963</td>\n",
       "      <td>39.631969</td>\n",
       "      <td>39.267342</td>\n",
       "      <td>42.369058</td>\n",
       "      <td>20.196017</td>\n",
       "      <td>16.605788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>11.420306</td>\n",
       "      <td>26.538798</td>\n",
       "      <td>24.760044</td>\n",
       "      <td>4.705844</td>\n",
       "      <td>22.940839</td>\n",
       "      <td>6.945770</td>\n",
       "      <td>25.129360</td>\n",
       "      <td>10.080399</td>\n",
       "      <td>11.357079</td>\n",
       "      <td>3.952258</td>\n",
       "      <td>...</td>\n",
       "      <td>14.982179</td>\n",
       "      <td>0.504640</td>\n",
       "      <td>23.023977</td>\n",
       "      <td>20.194849</td>\n",
       "      <td>14.562570</td>\n",
       "      <td>22.898446</td>\n",
       "      <td>22.717096</td>\n",
       "      <td>24.526511</td>\n",
       "      <td>11.692278</td>\n",
       "      <td>9.597830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.001579</td>\n",
       "      <td>0.002765</td>\n",
       "      <td>0.001114</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.000181</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000499</td>\n",
       "      <td>0.000211</td>\n",
       "      <td>0.000515</td>\n",
       "      <td>0.000537</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000710</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>0.000355</td>\n",
       "      <td>0.000238</td>\n",
       "      <td>0.000125</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.000861</td>\n",
       "      <td>0.000649</td>\n",
       "      <td>0.000595</td>\n",
       "      <td>0.000657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>9.876810</td>\n",
       "      <td>23.044580</td>\n",
       "      <td>21.399968</td>\n",
       "      <td>4.045547</td>\n",
       "      <td>19.923214</td>\n",
       "      <td>6.029313</td>\n",
       "      <td>21.778948</td>\n",
       "      <td>8.728659</td>\n",
       "      <td>9.850056</td>\n",
       "      <td>3.413296</td>\n",
       "      <td>...</td>\n",
       "      <td>12.960159</td>\n",
       "      <td>0.439992</td>\n",
       "      <td>19.848774</td>\n",
       "      <td>17.352205</td>\n",
       "      <td>12.804115</td>\n",
       "      <td>19.777779</td>\n",
       "      <td>19.559237</td>\n",
       "      <td>20.982007</td>\n",
       "      <td>10.065367</td>\n",
       "      <td>8.294438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>19.795351</td>\n",
       "      <td>46.160818</td>\n",
       "      <td>42.720351</td>\n",
       "      <td>8.144240</td>\n",
       "      <td>39.705752</td>\n",
       "      <td>12.034328</td>\n",
       "      <td>43.697342</td>\n",
       "      <td>17.441851</td>\n",
       "      <td>19.683303</td>\n",
       "      <td>6.848590</td>\n",
       "      <td>...</td>\n",
       "      <td>25.892752</td>\n",
       "      <td>0.880012</td>\n",
       "      <td>39.701756</td>\n",
       "      <td>34.781714</td>\n",
       "      <td>25.306480</td>\n",
       "      <td>39.785349</td>\n",
       "      <td>39.263032</td>\n",
       "      <td>42.506807</td>\n",
       "      <td>20.228040</td>\n",
       "      <td>16.607866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>29.667357</td>\n",
       "      <td>68.986637</td>\n",
       "      <td>64.320598</td>\n",
       "      <td>12.206382</td>\n",
       "      <td>59.435630</td>\n",
       "      <td>18.062989</td>\n",
       "      <td>65.321451</td>\n",
       "      <td>26.201945</td>\n",
       "      <td>29.539296</td>\n",
       "      <td>10.273923</td>\n",
       "      <td>...</td>\n",
       "      <td>38.952819</td>\n",
       "      <td>1.314513</td>\n",
       "      <td>59.760552</td>\n",
       "      <td>52.315396</td>\n",
       "      <td>37.957108</td>\n",
       "      <td>59.311149</td>\n",
       "      <td>58.769058</td>\n",
       "      <td>63.618503</td>\n",
       "      <td>30.304817</td>\n",
       "      <td>24.919081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>39.620228</td>\n",
       "      <td>92.085579</td>\n",
       "      <td>85.570883</td>\n",
       "      <td>16.266999</td>\n",
       "      <td>79.383920</td>\n",
       "      <td>24.076289</td>\n",
       "      <td>87.050661</td>\n",
       "      <td>34.884663</td>\n",
       "      <td>39.357229</td>\n",
       "      <td>13.685279</td>\n",
       "      <td>...</td>\n",
       "      <td>51.827134</td>\n",
       "      <td>1.752801</td>\n",
       "      <td>79.734872</td>\n",
       "      <td>69.882915</td>\n",
       "      <td>50.577111</td>\n",
       "      <td>79.361110</td>\n",
       "      <td>78.689761</td>\n",
       "      <td>84.782527</td>\n",
       "      <td>40.442033</td>\n",
       "      <td>33.241679</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  0              1              2              3   \\\n",
       "count  100000.000000  100000.000000  100000.000000  100000.000000   \n",
       "mean       19.782907      46.090433      42.772808       8.126919   \n",
       "std        11.420306      26.538798      24.760044       4.705844   \n",
       "min         0.001579       0.002765       0.001114       0.000056   \n",
       "25%         9.876810      23.044580      21.399968       4.045547   \n",
       "50%        19.795351      46.160818      42.720351       8.144240   \n",
       "75%        29.667357      68.986637      64.320598      12.206382   \n",
       "max        39.620228      92.085579      85.570883      16.266999   \n",
       "\n",
       "                  4              5              6              7   \\\n",
       "count  100000.000000  100000.000000  100000.000000  100000.000000   \n",
       "mean       39.682211      12.045670      43.537720      17.445476   \n",
       "std        22.940839       6.945770      25.129360      10.080399   \n",
       "min         0.000181       0.000061       0.000499       0.000211   \n",
       "25%        19.923214       6.029313      21.778948       8.728659   \n",
       "50%        39.705752      12.034328      43.697342      17.441851   \n",
       "75%        59.435630      18.062989      65.321451      26.201945   \n",
       "max        79.383920      24.076289      87.050661      34.884663   \n",
       "\n",
       "                  8              9   ...             20             21  \\\n",
       "count  100000.000000  100000.000000  ...  100000.000000  100000.000000   \n",
       "mean       19.697835       6.843953  ...      25.922545       0.877013   \n",
       "std        11.357079       3.952258  ...      14.982179       0.504640   \n",
       "min         0.000515       0.000537  ...       0.000710       0.000038   \n",
       "25%         9.850056       3.413296  ...      12.960159       0.439992   \n",
       "50%        19.683303       6.848590  ...      25.892752       0.880012   \n",
       "75%        29.539296      10.273923  ...      38.952819       1.314513   \n",
       "max        39.357229      13.685279  ...      51.827134       1.752801   \n",
       "\n",
       "                  22             23             24             25  \\\n",
       "count  100000.000000  100000.000000  100000.000000  100000.000000   \n",
       "mean       39.781936      34.854689      25.343963      39.631969   \n",
       "std        23.023977      20.194849      14.562570      22.898446   \n",
       "min         0.000355       0.000238       0.000125       0.000052   \n",
       "25%        19.848774      17.352205      12.804115      19.777779   \n",
       "50%        39.701756      34.781714      25.306480      39.785349   \n",
       "75%        59.760552      52.315396      37.957108      59.311149   \n",
       "max        79.734872      69.882915      50.577111      79.361110   \n",
       "\n",
       "                  26             27             28             29  \n",
       "count  100000.000000  100000.000000  100000.000000  100000.000000  \n",
       "mean       39.267342      42.369058      20.196017      16.605788  \n",
       "std        22.717096      24.526511      11.692278       9.597830  \n",
       "min         0.000861       0.000649       0.000595       0.000657  \n",
       "25%        19.559237      20.982007      10.065367       8.294438  \n",
       "50%        39.263032      42.506807      20.228040      16.607866  \n",
       "75%        58.769058      63.618503      30.304817      24.919081  \n",
       "max        78.689761      84.782527      40.442033      33.241679  \n",
       "\n",
       "[8 rows x 30 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_training_feat = pd.DataFrame(training_feat)\n",
    "pd_training_feat.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.488111</td>\n",
       "      <td>0.492417</td>\n",
       "      <td>0.493129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.156564</td>\n",
       "      <td>0.154757</td>\n",
       "      <td>0.158468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.376602</td>\n",
       "      <td>0.383181</td>\n",
       "      <td>0.380438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.488199</td>\n",
       "      <td>0.491776</td>\n",
       "      <td>0.493241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.599458</td>\n",
       "      <td>0.602343</td>\n",
       "      <td>0.605946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   0              1              2\n",
       "count  100000.000000  100000.000000  100000.000000\n",
       "mean        0.488111       0.492417       0.493129\n",
       "std         0.156564       0.154757       0.158468\n",
       "min         0.000000       0.000000       0.000000\n",
       "25%         0.376602       0.383181       0.380438\n",
       "50%         0.488199       0.491776       0.493241\n",
       "75%         0.599458       0.602343       0.605946\n",
       "max         1.000000       1.000000       1.000000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_training_true = pd.DataFrame(training_true)\n",
    "pd_training_true.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>65536.000000</td>\n",
       "      <td>65536.000000</td>\n",
       "      <td>65536.000000</td>\n",
       "      <td>65536.000000</td>\n",
       "      <td>65536.000000</td>\n",
       "      <td>65536.000000</td>\n",
       "      <td>65536.000000</td>\n",
       "      <td>65536.000000</td>\n",
       "      <td>65536.000000</td>\n",
       "      <td>65536.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>65536.000000</td>\n",
       "      <td>65536.000000</td>\n",
       "      <td>65536.000000</td>\n",
       "      <td>65536.000000</td>\n",
       "      <td>65536.000000</td>\n",
       "      <td>65536.000000</td>\n",
       "      <td>65536.000000</td>\n",
       "      <td>65536.000000</td>\n",
       "      <td>65536.000000</td>\n",
       "      <td>65536.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.454462</td>\n",
       "      <td>74.067802</td>\n",
       "      <td>44.857502</td>\n",
       "      <td>10.751527</td>\n",
       "      <td>53.081473</td>\n",
       "      <td>9.529815</td>\n",
       "      <td>17.654575</td>\n",
       "      <td>24.407069</td>\n",
       "      <td>5.338197</td>\n",
       "      <td>6.033054</td>\n",
       "      <td>...</td>\n",
       "      <td>37.387017</td>\n",
       "      <td>0.397624</td>\n",
       "      <td>43.320919</td>\n",
       "      <td>6.054117</td>\n",
       "      <td>39.017395</td>\n",
       "      <td>43.488805</td>\n",
       "      <td>27.581707</td>\n",
       "      <td>28.857502</td>\n",
       "      <td>27.298527</td>\n",
       "      <td>26.303207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>7.153041</td>\n",
       "      <td>17.097353</td>\n",
       "      <td>7.521236</td>\n",
       "      <td>2.300407</td>\n",
       "      <td>12.401176</td>\n",
       "      <td>3.031889</td>\n",
       "      <td>16.382879</td>\n",
       "      <td>6.787110</td>\n",
       "      <td>4.960646</td>\n",
       "      <td>1.113375</td>\n",
       "      <td>...</td>\n",
       "      <td>8.396434</td>\n",
       "      <td>0.325775</td>\n",
       "      <td>6.768171</td>\n",
       "      <td>10.282923</td>\n",
       "      <td>7.429106</td>\n",
       "      <td>11.945509</td>\n",
       "      <td>7.738622</td>\n",
       "      <td>7.772390</td>\n",
       "      <td>4.468032</td>\n",
       "      <td>4.559016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.205145</td>\n",
       "      <td>0.140010</td>\n",
       "      <td>0.007704</td>\n",
       "      <td>0.106029</td>\n",
       "      <td>0.826440</td>\n",
       "      <td>0.006542</td>\n",
       "      <td>0.116289</td>\n",
       "      <td>0.019159</td>\n",
       "      <td>0.054980</td>\n",
       "      <td>0.020258</td>\n",
       "      <td>...</td>\n",
       "      <td>0.086026</td>\n",
       "      <td>0.012425</td>\n",
       "      <td>0.079528</td>\n",
       "      <td>0.141013</td>\n",
       "      <td>0.534217</td>\n",
       "      <td>0.136502</td>\n",
       "      <td>0.251137</td>\n",
       "      <td>1.260045</td>\n",
       "      <td>0.274257</td>\n",
       "      <td>0.086213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.671194</td>\n",
       "      <td>80.687513</td>\n",
       "      <td>46.080703</td>\n",
       "      <td>11.513736</td>\n",
       "      <td>57.172443</td>\n",
       "      <td>8.511868</td>\n",
       "      <td>11.549045</td>\n",
       "      <td>26.954927</td>\n",
       "      <td>4.106729</td>\n",
       "      <td>6.053575</td>\n",
       "      <td>...</td>\n",
       "      <td>40.397061</td>\n",
       "      <td>0.275670</td>\n",
       "      <td>44.725183</td>\n",
       "      <td>2.465425</td>\n",
       "      <td>41.724439</td>\n",
       "      <td>41.317566</td>\n",
       "      <td>25.968841</td>\n",
       "      <td>27.969908</td>\n",
       "      <td>27.371512</td>\n",
       "      <td>27.943945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.671194</td>\n",
       "      <td>80.687513</td>\n",
       "      <td>46.080703</td>\n",
       "      <td>11.513736</td>\n",
       "      <td>57.172443</td>\n",
       "      <td>8.511868</td>\n",
       "      <td>11.549045</td>\n",
       "      <td>26.954927</td>\n",
       "      <td>4.106729</td>\n",
       "      <td>6.053575</td>\n",
       "      <td>...</td>\n",
       "      <td>40.397061</td>\n",
       "      <td>0.275670</td>\n",
       "      <td>44.725183</td>\n",
       "      <td>2.465425</td>\n",
       "      <td>41.724439</td>\n",
       "      <td>41.317566</td>\n",
       "      <td>25.968841</td>\n",
       "      <td>27.969908</td>\n",
       "      <td>27.371512</td>\n",
       "      <td>27.943945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.671194</td>\n",
       "      <td>80.687513</td>\n",
       "      <td>46.080703</td>\n",
       "      <td>11.513736</td>\n",
       "      <td>57.172443</td>\n",
       "      <td>8.511868</td>\n",
       "      <td>11.549045</td>\n",
       "      <td>26.954927</td>\n",
       "      <td>4.106729</td>\n",
       "      <td>6.053575</td>\n",
       "      <td>...</td>\n",
       "      <td>40.397061</td>\n",
       "      <td>0.275670</td>\n",
       "      <td>44.725183</td>\n",
       "      <td>2.465425</td>\n",
       "      <td>41.724439</td>\n",
       "      <td>41.317566</td>\n",
       "      <td>25.968841</td>\n",
       "      <td>27.969908</td>\n",
       "      <td>27.371512</td>\n",
       "      <td>27.943945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>39.270873</td>\n",
       "      <td>91.960012</td>\n",
       "      <td>85.403592</td>\n",
       "      <td>16.114298</td>\n",
       "      <td>79.153604</td>\n",
       "      <td>23.595508</td>\n",
       "      <td>86.511252</td>\n",
       "      <td>34.759464</td>\n",
       "      <td>38.897368</td>\n",
       "      <td>13.670417</td>\n",
       "      <td>...</td>\n",
       "      <td>50.860761</td>\n",
       "      <td>1.743625</td>\n",
       "      <td>79.640676</td>\n",
       "      <td>69.227956</td>\n",
       "      <td>50.451757</td>\n",
       "      <td>77.844461</td>\n",
       "      <td>78.319494</td>\n",
       "      <td>84.625035</td>\n",
       "      <td>40.362388</td>\n",
       "      <td>33.192081</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 0             1             2             3             4   \\\n",
       "count  65536.000000  65536.000000  65536.000000  65536.000000  65536.000000   \n",
       "mean       3.454462     74.067802     44.857502     10.751527     53.081473   \n",
       "std        7.153041     17.097353      7.521236      2.300407     12.401176   \n",
       "min        0.205145      0.140010      0.007704      0.106029      0.826440   \n",
       "25%        0.671194     80.687513     46.080703     11.513736     57.172443   \n",
       "50%        0.671194     80.687513     46.080703     11.513736     57.172443   \n",
       "75%        0.671194     80.687513     46.080703     11.513736     57.172443   \n",
       "max       39.270873     91.960012     85.403592     16.114298     79.153604   \n",
       "\n",
       "                 5             6             7             8             9   \\\n",
       "count  65536.000000  65536.000000  65536.000000  65536.000000  65536.000000   \n",
       "mean       9.529815     17.654575     24.407069      5.338197      6.033054   \n",
       "std        3.031889     16.382879      6.787110      4.960646      1.113375   \n",
       "min        0.006542      0.116289      0.019159      0.054980      0.020258   \n",
       "25%        8.511868     11.549045     26.954927      4.106729      6.053575   \n",
       "50%        8.511868     11.549045     26.954927      4.106729      6.053575   \n",
       "75%        8.511868     11.549045     26.954927      4.106729      6.053575   \n",
       "max       23.595508     86.511252     34.759464     38.897368     13.670417   \n",
       "\n",
       "       ...            20            21            22            23  \\\n",
       "count  ...  65536.000000  65536.000000  65536.000000  65536.000000   \n",
       "mean   ...     37.387017      0.397624     43.320919      6.054117   \n",
       "std    ...      8.396434      0.325775      6.768171     10.282923   \n",
       "min    ...      0.086026      0.012425      0.079528      0.141013   \n",
       "25%    ...     40.397061      0.275670     44.725183      2.465425   \n",
       "50%    ...     40.397061      0.275670     44.725183      2.465425   \n",
       "75%    ...     40.397061      0.275670     44.725183      2.465425   \n",
       "max    ...     50.860761      1.743625     79.640676     69.227956   \n",
       "\n",
       "                 24            25            26            27            28  \\\n",
       "count  65536.000000  65536.000000  65536.000000  65536.000000  65536.000000   \n",
       "mean      39.017395     43.488805     27.581707     28.857502     27.298527   \n",
       "std        7.429106     11.945509      7.738622      7.772390      4.468032   \n",
       "min        0.534217      0.136502      0.251137      1.260045      0.274257   \n",
       "25%       41.724439     41.317566     25.968841     27.969908     27.371512   \n",
       "50%       41.724439     41.317566     25.968841     27.969908     27.371512   \n",
       "75%       41.724439     41.317566     25.968841     27.969908     27.371512   \n",
       "max       50.451757     77.844461     78.319494     84.625035     40.362388   \n",
       "\n",
       "                 29  \n",
       "count  65536.000000  \n",
       "mean      26.303207  \n",
       "std        4.559016  \n",
       "min        0.086213  \n",
       "25%       27.943945  \n",
       "50%       27.943945  \n",
       "75%       27.943945  \n",
       "max       33.192081  \n",
       "\n",
       "[8 rows x 30 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_validation_feat = pd.DataFrame(validation_feat)\n",
    "pd_validation_feat.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feat_scaled = StandardScaler().fit_transform(training_feat)\n",
    "#train_feat_scaled = MinMaxScaler().fit_transform(training_feat)\n",
    "train_true_scaled = MinMaxScaler().fit_transform(training_true)\n",
    "validation_feat_scaled = StandardScaler().fit_transform(validation_feat)\n",
    "#validation_feat_scaled = MinMaxScaler().fit_transform(validation_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.000000e+05</td>\n",
       "      <td>1.000000e+05</td>\n",
       "      <td>1.000000e+05</td>\n",
       "      <td>1.000000e+05</td>\n",
       "      <td>1.000000e+05</td>\n",
       "      <td>1.000000e+05</td>\n",
       "      <td>1.000000e+05</td>\n",
       "      <td>1.000000e+05</td>\n",
       "      <td>1.000000e+05</td>\n",
       "      <td>1.000000e+05</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000e+05</td>\n",
       "      <td>1.000000e+05</td>\n",
       "      <td>1.000000e+05</td>\n",
       "      <td>1.000000e+05</td>\n",
       "      <td>1.000000e+05</td>\n",
       "      <td>1.000000e+05</td>\n",
       "      <td>1.000000e+05</td>\n",
       "      <td>1.000000e+05</td>\n",
       "      <td>1.000000e+05</td>\n",
       "      <td>1.000000e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.653100e-16</td>\n",
       "      <td>-6.528778e-17</td>\n",
       "      <td>4.543882e-16</td>\n",
       "      <td>-8.614331e-17</td>\n",
       "      <td>-3.015832e-16</td>\n",
       "      <td>-2.353073e-16</td>\n",
       "      <td>-5.550449e-17</td>\n",
       "      <td>-4.165557e-18</td>\n",
       "      <td>-6.577405e-17</td>\n",
       "      <td>9.258816e-17</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.071834e-16</td>\n",
       "      <td>-2.781519e-16</td>\n",
       "      <td>-4.368872e-16</td>\n",
       "      <td>1.129030e-16</td>\n",
       "      <td>-2.371281e-16</td>\n",
       "      <td>-8.577583e-17</td>\n",
       "      <td>-2.136802e-16</td>\n",
       "      <td>-3.516409e-16</td>\n",
       "      <td>1.580336e-16</td>\n",
       "      <td>-2.420630e-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.000005e+00</td>\n",
       "      <td>1.000005e+00</td>\n",
       "      <td>1.000005e+00</td>\n",
       "      <td>1.000005e+00</td>\n",
       "      <td>1.000005e+00</td>\n",
       "      <td>1.000005e+00</td>\n",
       "      <td>1.000005e+00</td>\n",
       "      <td>1.000005e+00</td>\n",
       "      <td>1.000005e+00</td>\n",
       "      <td>1.000005e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000005e+00</td>\n",
       "      <td>1.000005e+00</td>\n",
       "      <td>1.000005e+00</td>\n",
       "      <td>1.000005e+00</td>\n",
       "      <td>1.000005e+00</td>\n",
       "      <td>1.000005e+00</td>\n",
       "      <td>1.000005e+00</td>\n",
       "      <td>1.000005e+00</td>\n",
       "      <td>1.000005e+00</td>\n",
       "      <td>1.000005e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-1.732128e+00</td>\n",
       "      <td>-1.736623e+00</td>\n",
       "      <td>-1.727457e+00</td>\n",
       "      <td>-1.726981e+00</td>\n",
       "      <td>-1.729764e+00</td>\n",
       "      <td>-1.734245e+00</td>\n",
       "      <td>-1.732533e+00</td>\n",
       "      <td>-1.730621e+00</td>\n",
       "      <td>-1.734374e+00</td>\n",
       "      <td>-1.731529e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.730187e+00</td>\n",
       "      <td>-1.737831e+00</td>\n",
       "      <td>-1.727841e+00</td>\n",
       "      <td>-1.725917e+00</td>\n",
       "      <td>-1.740350e+00</td>\n",
       "      <td>-1.730778e+00</td>\n",
       "      <td>-1.728508e+00</td>\n",
       "      <td>-1.727462e+00</td>\n",
       "      <td>-1.727253e+00</td>\n",
       "      <td>-1.730101e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-8.674152e-01</td>\n",
       "      <td>-8.683878e-01</td>\n",
       "      <td>-8.632031e-01</td>\n",
       "      <td>-8.673030e-01</td>\n",
       "      <td>-8.613066e-01</td>\n",
       "      <td>-8.661944e-01</td>\n",
       "      <td>-8.658749e-01</td>\n",
       "      <td>-8.647337e-01</td>\n",
       "      <td>-8.671093e-01</td>\n",
       "      <td>-8.680289e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>-8.651913e-01</td>\n",
       "      <td>-8.660091e-01</td>\n",
       "      <td>-8.657610e-01</td>\n",
       "      <td>-8.666850e-01</td>\n",
       "      <td>-8.611056e-01</td>\n",
       "      <td>-8.670584e-01</td>\n",
       "      <td>-8.675494e-01</td>\n",
       "      <td>-8.720016e-01</td>\n",
       "      <td>-8.664437e-01</td>\n",
       "      <td>-8.659657e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.089687e-03</td>\n",
       "      <td>2.652156e-03</td>\n",
       "      <td>-2.118596e-03</td>\n",
       "      <td>3.680685e-03</td>\n",
       "      <td>1.026158e-03</td>\n",
       "      <td>-1.632950e-03</td>\n",
       "      <td>6.352037e-03</td>\n",
       "      <td>-3.596176e-04</td>\n",
       "      <td>-1.279599e-03</td>\n",
       "      <td>1.173276e-03</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.988549e-03</td>\n",
       "      <td>5.943499e-03</td>\n",
       "      <td>-3.482499e-03</td>\n",
       "      <td>-3.613576e-03</td>\n",
       "      <td>-2.573903e-03</td>\n",
       "      <td>6.698311e-03</td>\n",
       "      <td>-1.897305e-04</td>\n",
       "      <td>5.616360e-03</td>\n",
       "      <td>2.738790e-03</td>\n",
       "      <td>2.164662e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>8.655197e-01</td>\n",
       "      <td>8.627489e-01</td>\n",
       "      <td>8.702690e-01</td>\n",
       "      <td>8.668973e-01</td>\n",
       "      <td>8.610634e-01</td>\n",
       "      <td>8.663328e-01</td>\n",
       "      <td>8.668680e-01</td>\n",
       "      <td>8.686673e-01</td>\n",
       "      <td>8.665529e-01</td>\n",
       "      <td>8.678550e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>8.697225e-01</td>\n",
       "      <td>8.669575e-01</td>\n",
       "      <td>8.677352e-01</td>\n",
       "      <td>8.646162e-01</td>\n",
       "      <td>8.661389e-01</td>\n",
       "      <td>8.594154e-01</td>\n",
       "      <td>8.584642e-01</td>\n",
       "      <td>8.663912e-01</td>\n",
       "      <td>8.645750e-01</td>\n",
       "      <td>8.661680e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.737031e+00</td>\n",
       "      <td>1.733137e+00</td>\n",
       "      <td>1.728522e+00</td>\n",
       "      <td>1.729790e+00</td>\n",
       "      <td>1.730621e+00</td>\n",
       "      <td>1.732087e+00</td>\n",
       "      <td>1.731567e+00</td>\n",
       "      <td>1.730018e+00</td>\n",
       "      <td>1.731034e+00</td>\n",
       "      <td>1.731000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>1.729035e+00</td>\n",
       "      <td>1.735479e+00</td>\n",
       "      <td>1.735284e+00</td>\n",
       "      <td>1.734522e+00</td>\n",
       "      <td>1.732749e+00</td>\n",
       "      <td>1.735023e+00</td>\n",
       "      <td>1.735372e+00</td>\n",
       "      <td>1.729299e+00</td>\n",
       "      <td>1.731580e+00</td>\n",
       "      <td>1.733306e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 0             1             2             3             4   \\\n",
       "count  1.000000e+05  1.000000e+05  1.000000e+05  1.000000e+05  1.000000e+05   \n",
       "mean   3.653100e-16 -6.528778e-17  4.543882e-16 -8.614331e-17 -3.015832e-16   \n",
       "std    1.000005e+00  1.000005e+00  1.000005e+00  1.000005e+00  1.000005e+00   \n",
       "min   -1.732128e+00 -1.736623e+00 -1.727457e+00 -1.726981e+00 -1.729764e+00   \n",
       "25%   -8.674152e-01 -8.683878e-01 -8.632031e-01 -8.673030e-01 -8.613066e-01   \n",
       "50%    1.089687e-03  2.652156e-03 -2.118596e-03  3.680685e-03  1.026158e-03   \n",
       "75%    8.655197e-01  8.627489e-01  8.702690e-01  8.668973e-01  8.610634e-01   \n",
       "max    1.737031e+00  1.733137e+00  1.728522e+00  1.729790e+00  1.730621e+00   \n",
       "\n",
       "                 5             6             7             8             9   \\\n",
       "count  1.000000e+05  1.000000e+05  1.000000e+05  1.000000e+05  1.000000e+05   \n",
       "mean  -2.353073e-16 -5.550449e-17 -4.165557e-18 -6.577405e-17  9.258816e-17   \n",
       "std    1.000005e+00  1.000005e+00  1.000005e+00  1.000005e+00  1.000005e+00   \n",
       "min   -1.734245e+00 -1.732533e+00 -1.730621e+00 -1.734374e+00 -1.731529e+00   \n",
       "25%   -8.661944e-01 -8.658749e-01 -8.647337e-01 -8.671093e-01 -8.680289e-01   \n",
       "50%   -1.632950e-03  6.352037e-03 -3.596176e-04 -1.279599e-03  1.173276e-03   \n",
       "75%    8.663328e-01  8.668680e-01  8.686673e-01  8.665529e-01  8.678550e-01   \n",
       "max    1.732087e+00  1.731567e+00  1.730018e+00  1.731034e+00  1.731000e+00   \n",
       "\n",
       "       ...            20            21            22            23  \\\n",
       "count  ...  1.000000e+05  1.000000e+05  1.000000e+05  1.000000e+05   \n",
       "mean   ... -2.071834e-16 -2.781519e-16 -4.368872e-16  1.129030e-16   \n",
       "std    ...  1.000005e+00  1.000005e+00  1.000005e+00  1.000005e+00   \n",
       "min    ... -1.730187e+00 -1.737831e+00 -1.727841e+00 -1.725917e+00   \n",
       "25%    ... -8.651913e-01 -8.660091e-01 -8.657610e-01 -8.666850e-01   \n",
       "50%    ... -1.988549e-03  5.943499e-03 -3.482499e-03 -3.613576e-03   \n",
       "75%    ...  8.697225e-01  8.669575e-01  8.677352e-01  8.646162e-01   \n",
       "max    ...  1.729035e+00  1.735479e+00  1.735284e+00  1.734522e+00   \n",
       "\n",
       "                 24            25            26            27            28  \\\n",
       "count  1.000000e+05  1.000000e+05  1.000000e+05  1.000000e+05  1.000000e+05   \n",
       "mean  -2.371281e-16 -8.577583e-17 -2.136802e-16 -3.516409e-16  1.580336e-16   \n",
       "std    1.000005e+00  1.000005e+00  1.000005e+00  1.000005e+00  1.000005e+00   \n",
       "min   -1.740350e+00 -1.730778e+00 -1.728508e+00 -1.727462e+00 -1.727253e+00   \n",
       "25%   -8.611056e-01 -8.670584e-01 -8.675494e-01 -8.720016e-01 -8.664437e-01   \n",
       "50%   -2.573903e-03  6.698311e-03 -1.897305e-04  5.616360e-03  2.738790e-03   \n",
       "75%    8.661389e-01  8.594154e-01  8.584642e-01  8.663912e-01  8.645750e-01   \n",
       "max    1.732749e+00  1.735023e+00  1.735372e+00  1.729299e+00  1.731580e+00   \n",
       "\n",
       "                 29  \n",
       "count  1.000000e+05  \n",
       "mean  -2.420630e-16  \n",
       "std    1.000005e+00  \n",
       "min   -1.730101e+00  \n",
       "25%   -8.659657e-01  \n",
       "50%    2.164662e-04  \n",
       "75%    8.661680e-01  \n",
       "max    1.733306e+00  \n",
       "\n",
       "[8 rows x 30 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_train_feat_scaled = pd.DataFrame(train_feat_scaled)\n",
    "pd_train_feat_scaled.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Dataset (5 points)\n",
    "\n",
    "When training neural networks it is important to build a dataset that allows the machinery to sample the data. This also can be used to conduct some preprocessing of the data to make it work with PyTorch. \n",
    "\n",
    "I have provided you with the framework for a Dataset Class. \n",
    "\n",
    "You should:\n",
    "1. Convert the x and y data to a tensor 'float32' and put it on the GPU.\n",
    "2. Save the len of the data\n",
    "3. Add the code so when `__getitem__` is called it returns the x and y values\n",
    "3. make it so `__len__` returns the lenght when calle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "class Data(Dataset):\n",
    "  '''Dataset Class to store the samples and their corresponding labels, \n",
    "  and DataLoader wraps an iterable around the Dataset to enable easy access to the samples.\n",
    "  '''\n",
    "\n",
    "  def __init__(self, X: np.ndarray, y: np.ndarray, device = 'cpu') -> None:\n",
    "\n",
    "    # need to convert float64 to float32 else \n",
    "    # will get the following error\n",
    "    # RuntimeError: expected scalar type Double but found Float\n",
    "    X = np.float32(X)\n",
    "    self.X = torch.from_numpy(X).to(device)\n",
    "    y = np.float32(y)\n",
    "    self.y = torch.from_numpy(y).to(device)\n",
    "    self.len = self.X.shape[0]\n",
    "  \n",
    "  def __getitem__(self, index: int) -> tuple:\n",
    "    return self.X[index], self.y[index]\n",
    "\n",
    "  def __len__(self) -> int:\n",
    "    return self.len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-test Split (3 points)\n",
    "\n",
    "1. You should conduct a train-test split of the training data so you can make sure that your model does not overfit the data. A good ratio is 66/33 train \n",
    "2. You should instantiate the training dataset using the data class implemented above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test=train_test_split(train_feat_scaled, train_true_scaled,test_size=0.33)\n",
    "\n",
    "training_data = Data(X_train,y_train)\n",
    "testing_data= Data(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Dataloader (3 points)\n",
    "\n",
    "Pytorch uses DataLoaders to efficiently sample from a training dataset. Instantiate a Pytorch DataLoader using the dataset. \n",
    "\n",
    "You should set the following parameters:\n",
    "1. Batch size = 64\n",
    "2. Shuffle = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(training_data, batch_size=64,shuffle=True)\n",
    "test_dataloader = DataLoader(testing_data, batch_size=64,shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Neural Network (5 points)\n",
    "\n",
    "Using the provided class framework which inherits the `nn.Module` type in PyTorch builds a 4-layer neural network to complete the multiple regression.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Neural_Network(nn.Module):\n",
    "    ''' Regression Model\n",
    "    ''' \n",
    "\n",
    "  # note, you can ignore the `:int` and `-> None` this is just more advanced doctring syntax\n",
    "    def __init__(self, input_dim: int, hidden_dim: int, output_dim: int) -> None:\n",
    "\n",
    "        super(Neural_Network, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim),\n",
    "            )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "      # In this part you should build a model that returns the 3 outputs of the regression\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate the Model (3 points)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "# number of features (len of X cols)\n",
    "input_dim = X_train.shape[1]\n",
    "# number of hidden layers set this to 50\n",
    "hidden_layers = 50\n",
    "# Add the number of output dimensions\n",
    "output_dim = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural_Network(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=30, out_features=50, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=50, out_features=3, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# initiate the regression model\n",
    "# make sure to put it on your GPU\n",
    "model = Neural_Network(input_dim,hidden_layers,output_dim).to(device)\n",
    "print(model)\n",
    "\n",
    "# criterion to computes the loss between input and target\n",
    "# Choose a good criteria\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# optimizer that will be used to update weights and biases\n",
    "# you can choose any optimizer. I would recommend ADAM.\n",
    "# This problem should not be hard to optimize. A good starting learning rate is 3e-5. \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model structure: Neural_Network(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=30, out_features=50, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=50, out_features=3, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model structure: {model}\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model (5 points)\n",
    "\n",
    "Training the model is conducted in a number of steps using loops.\n",
    "\n",
    "1. Set up a loop for each epoch\n",
    "2. Set a parameter to save the running loss\n",
    "3. Set up a nested loop that goes through the batches from the DataLoader you built\n",
    "    - I would recommend using enumerate to include the counts in the loop\n",
    "    - The dataloader will return a tuple that is the inputs and the labels\n",
    "4. Conduct the forward propagation of the model\n",
    "    - Give the model the inputs and compute the outputs\n",
    "    - Compute the loss given the criteria. \n",
    "5. Use the zero gradient method to remove the gradients from the optimizer\n",
    "6. Use the backward method to compute the gradients\n",
    "7. Use the step method in the optimizer to take an optimization step\n",
    "8. Compute the running loss by calling the item method and adding it to the running loss for each minibatch\n",
    "9. For each epoch print the epoch and the loss\n",
    "\n",
    "Note: If you find this challenging I would recommend that you look at examples of other pytorch training loops online. This is a very standard workflow. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, TensorDataset\n",
    "\n",
    "batch_size=64\n",
    "train_combined = TensorDataset(torch.stack([torch.Tensor(n) for n in X_train]), torch.stack([torch.Tensor(n) for n in y_train]))\n",
    "train_dataloader1 = DataLoader(train_combined, batch_size=batch_size, shuffle=True)\n",
    "test_combined = TensorDataset(torch.stack([torch.Tensor(n) for n in X_test]), torch.stack([torch.Tensor(n) for n in y_test]))\n",
    "test_dataloader1 = DataLoader(test_combined, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 0.233890  [    0/67000]\n",
      "loss: 0.236259  [ 6400/67000]\n",
      "loss: 0.190895  [12800/67000]\n",
      "loss: 0.113618  [19200/67000]\n",
      "loss: 0.060434  [25600/67000]\n",
      "loss: 0.036948  [32000/67000]\n",
      "loss: 0.027442  [38400/67000]\n",
      "loss: 0.026803  [44800/67000]\n",
      "loss: 0.023960  [51200/67000]\n",
      "loss: 0.023588  [57600/67000]\n",
      "loss: 0.024826  [64000/67000]\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.020816  [    0/67000]\n",
      "loss: 0.020463  [ 6400/67000]\n",
      "loss: 0.018550  [12800/67000]\n",
      "loss: 0.016834  [19200/67000]\n",
      "loss: 0.019281  [25600/67000]\n",
      "loss: 0.017851  [32000/67000]\n",
      "loss: 0.015232  [38400/67000]\n",
      "loss: 0.015201  [44800/67000]\n",
      "loss: 0.011906  [51200/67000]\n",
      "loss: 0.015461  [57600/67000]\n",
      "loss: 0.013117  [64000/67000]\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.013608  [    0/67000]\n",
      "loss: 0.012718  [ 6400/67000]\n",
      "loss: 0.012975  [12800/67000]\n",
      "loss: 0.014201  [19200/67000]\n",
      "loss: 0.012215  [25600/67000]\n",
      "loss: 0.011141  [32000/67000]\n",
      "loss: 0.009778  [38400/67000]\n",
      "loss: 0.010814  [44800/67000]\n",
      "loss: 0.013037  [51200/67000]\n",
      "loss: 0.011435  [57600/67000]\n",
      "loss: 0.010926  [64000/67000]\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.012290  [    0/67000]\n",
      "loss: 0.010411  [ 6400/67000]\n",
      "loss: 0.010071  [12800/67000]\n",
      "loss: 0.009237  [19200/67000]\n",
      "loss: 0.009290  [25600/67000]\n",
      "loss: 0.010856  [32000/67000]\n",
      "loss: 0.012481  [38400/67000]\n",
      "loss: 0.008272  [44800/67000]\n",
      "loss: 0.011145  [51200/67000]\n",
      "loss: 0.008550  [57600/67000]\n",
      "loss: 0.010321  [64000/67000]\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.010264  [    0/67000]\n",
      "loss: 0.008940  [ 6400/67000]\n",
      "loss: 0.010782  [12800/67000]\n",
      "loss: 0.008042  [19200/67000]\n",
      "loss: 0.008099  [25600/67000]\n",
      "loss: 0.007873  [32000/67000]\n",
      "loss: 0.008866  [38400/67000]\n",
      "loss: 0.007208  [44800/67000]\n",
      "loss: 0.009236  [51200/67000]\n",
      "loss: 0.009297  [57600/67000]\n",
      "loss: 0.007749  [64000/67000]\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.010988  [    0/67000]\n",
      "loss: 0.009809  [ 6400/67000]\n",
      "loss: 0.007819  [12800/67000]\n",
      "loss: 0.009222  [19200/67000]\n",
      "loss: 0.009405  [25600/67000]\n",
      "loss: 0.009437  [32000/67000]\n",
      "loss: 0.006987  [38400/67000]\n",
      "loss: 0.007946  [44800/67000]\n",
      "loss: 0.008978  [51200/67000]\n",
      "loss: 0.008854  [57600/67000]\n",
      "loss: 0.008012  [64000/67000]\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.009090  [    0/67000]\n",
      "loss: 0.006701  [ 6400/67000]\n",
      "loss: 0.008478  [12800/67000]\n",
      "loss: 0.008920  [19200/67000]\n",
      "loss: 0.008485  [25600/67000]\n",
      "loss: 0.008511  [32000/67000]\n",
      "loss: 0.006877  [38400/67000]\n",
      "loss: 0.008517  [44800/67000]\n",
      "loss: 0.007229  [51200/67000]\n",
      "loss: 0.007928  [57600/67000]\n",
      "loss: 0.006815  [64000/67000]\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.007400  [    0/67000]\n",
      "loss: 0.009083  [ 6400/67000]\n",
      "loss: 0.008897  [12800/67000]\n",
      "loss: 0.007124  [19200/67000]\n",
      "loss: 0.006412  [25600/67000]\n",
      "loss: 0.007304  [32000/67000]\n",
      "loss: 0.007313  [38400/67000]\n",
      "loss: 0.006759  [44800/67000]\n",
      "loss: 0.007542  [51200/67000]\n",
      "loss: 0.005740  [57600/67000]\n",
      "loss: 0.008824  [64000/67000]\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.007619  [    0/67000]\n",
      "loss: 0.007309  [ 6400/67000]\n",
      "loss: 0.006365  [12800/67000]\n",
      "loss: 0.006128  [19200/67000]\n",
      "loss: 0.005916  [25600/67000]\n",
      "loss: 0.005835  [32000/67000]\n",
      "loss: 0.006913  [38400/67000]\n",
      "loss: 0.007097  [44800/67000]\n",
      "loss: 0.009232  [51200/67000]\n",
      "loss: 0.007567  [57600/67000]\n",
      "loss: 0.007496  [64000/67000]\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.006589  [    0/67000]\n",
      "loss: 0.007374  [ 6400/67000]\n",
      "loss: 0.007466  [12800/67000]\n",
      "loss: 0.006301  [19200/67000]\n",
      "loss: 0.005957  [25600/67000]\n",
      "loss: 0.008447  [32000/67000]\n",
      "loss: 0.006865  [38400/67000]\n",
      "loss: 0.006648  [44800/67000]\n",
      "loss: 0.007077  [51200/67000]\n",
      "loss: 0.006310  [57600/67000]\n",
      "loss: 0.007592  [64000/67000]\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 0.006093  [    0/67000]\n",
      "loss: 0.008348  [ 6400/67000]\n",
      "loss: 0.006309  [12800/67000]\n",
      "loss: 0.006378  [19200/67000]\n",
      "loss: 0.007105  [25600/67000]\n",
      "loss: 0.006735  [32000/67000]\n",
      "loss: 0.006406  [38400/67000]\n",
      "loss: 0.006684  [44800/67000]\n",
      "loss: 0.007797  [51200/67000]\n",
      "loss: 0.006462  [57600/67000]\n",
      "loss: 0.007110  [64000/67000]\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 0.006839  [    0/67000]\n",
      "loss: 0.006938  [ 6400/67000]\n",
      "loss: 0.007645  [12800/67000]\n",
      "loss: 0.005074  [19200/67000]\n",
      "loss: 0.006484  [25600/67000]\n",
      "loss: 0.006490  [32000/67000]\n",
      "loss: 0.007137  [38400/67000]\n",
      "loss: 0.006073  [44800/67000]\n",
      "loss: 0.006519  [51200/67000]\n",
      "loss: 0.006552  [57600/67000]\n"
     ]
    }
   ],
   "source": [
    "# start training\n",
    "epochs = 20 # sets the number of epochs to train 20 should be sufficent.\n",
    "# This should take about 5-10 minutes to train.\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    #test(test_dataloader1, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate the Model (3 points)\n",
    "\n",
    "Use the test dataset from the train-test split to make sure your model is not overfitting\n",
    "\n",
    "1. You can build a dataloader as you did before, this time with the test data.\n",
    "2. Build a validation loop, which you should use `with torch.no_grad()` to make sure you do not modify the gradients, or weights. This will fix your model. \n",
    "3. Instantiate the loss to be 0.\n",
    "4. Build a similar loop to grab the validation dataset. \n",
    "5. Compute the predictions with the model.\n",
    "6. Compute the loss using your loss criteria.\n",
    "7. Print the final loss determined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20 # sets the number of epochs to train 20 should be sufficent.\n",
    "# This should take about 5-10 minutes to train.\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    #train(train_dataloader1, model, loss_fn, optimizer)\n",
    "    test(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"model.pth\")\n",
    "print(\"Saved PyTorch Model State to model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color:blue\"> Question: Is your model overfitting or not? How do you know? (3 points) </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Type your response here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crack Your Code (3 points)\n",
    "\n",
    "1. You can build a dataloader as you did before, this time with the validation features to view your code.\n",
    "2. Build a loop, you should use `with torch.no_grad()` to make sure you do not modify the gradients or weights. This will fix your model. \n",
    "3. Compute the predictions of your model. \n",
    "    - Make sure you do all the same preprocess, the data has the same datatype, and is on the same device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data class for the validation features\n",
    "class Data_val(Dataset):\n",
    "  '''Dataset Class to store the samples and their corresponding labels, \n",
    "  and DataLoader wraps an iterable around the Dataset to enable easy access to the samples.\n",
    "  '''\n",
    "\n",
    "  def __init__(self, X: np.ndarray, device = 'cpu') -> None:\n",
    "\n",
    "    # need to convert float64 to float32 else \n",
    "    # will get the following error\n",
    "    # RuntimeError: expected scalar type Double but found Float\n",
    "    X = np.float32(X)\n",
    "    self.X = torch.from_numpy(X).to(device)# here\n",
    "    self.len = self.X.shape[0] # here\n",
    "  \n",
    "  def __getitem__(self, index: int) -> tuple:\n",
    "    return self.X[index] # here\n",
    "\n",
    "  def __len__(self) -> int:\n",
    "    return self.len #here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Builing dataloader for validation features\n",
    "validation_data = Data_val(validation_feat_scaled)\n",
    "val_dataloader = DataLoader(validation_data, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing predictions on validation features\n",
    "total_preds = torch.tensor([], device=device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch, X in enumerate (val_dataloader):\n",
    "        \n",
    "        predicted = model(X)\n",
    "        # inverse transform of the predictions\n",
    "        total_preds = torch.cat((total_preds, predicted), 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reveal Your Code (3 points)\n",
    "\n",
    "Your code is an image. there are (65536, 3) predictions this corresponds to a (256,256,3) RGB image. \n",
    "1. Use the detach() method to remove the gradients from the tensor\n",
    "2. Transfer the tensor back to the 'cpu' if you had it on a GPU\n",
    "3. Reshape the image into a 256,256,3 array. \n",
    "4. Plot your successful result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_true_scaler = StandardScaler()\n",
    "train_true_scaled2 = train_true_scaler.fit_transform(training_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detach() to remove the gradients\n",
    "# cpu() to transfer from GPU to CPU\n",
    "# Transform to numpy and rescale back. Reshape the image into (256, 256, 3) shape\n",
    "image = total_preds.cpu().detach().numpy()\n",
    "image = train_true_scaler.inverse_transform(image)\n",
    "\n",
    "image = image.reshape((256, 256, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the result\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(image)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "c475b5beda6d617ffb7b2fcf453fbe132321ffc1e1f96c06cf49356e1e7f42cb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
